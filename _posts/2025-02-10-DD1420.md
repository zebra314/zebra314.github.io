---
title: "DD1420 Updating"
header:
  teaser:
tag: Exchange
katex: True
---

課程名稱：Foundations of Machine Learning

## Lesson 5

### 幾何學

### Dimensionality reduction

將數據的維度降低，複雜度降低，我們能更容易的分析數據，當資料的維度降低到只剩下幾個抽象特徵時，可以加快模型的訓練速度和大小，且當維度降低至2或3維時，也能更直觀的視覺化數據。

假設某個產品有300個指標，這些指標反應了顧客的需求以及產品的特性，但這300個指標都有用嗎？會不會改善其中的20個指標就能達到差不多的效果了呢？

### Principal Component Analysis, PCA

主成份分析，一種經典的線性降維方法，目標是尋找一個最佳的線性正交投影 $\pi$，將一組 $d$ 維的數據降到 $k$ 維，$d > k$，使得投影後的數據在低維子空間中能夠保有最大的變異性。

這個子空間有 $k$ 維，由 $k$ 個正交的特徵向量 $u_1, \dots, u_k \in \mathbb{R}^n$ 張成，這些向量稱為主成分，滿足以下條件：

1. 正交：$u_i^T u_j = 0, i \neq j$，確保這些向量不會有重複的資訊，每個向量表示不同的方向。

2. 單位長度：$u_i^t u_i = \|u_i\|^2 = 1$，確保數據在投影後不會因為基底向量的長度變化而被放大或縮小。

變異性代表數據的分散程度，如果數據在某方向上的變異性較大，代表這個方向上的變化較多，不同的數據點有顯著區別，表示資訊較多、較豐富。

PCA有三個步驟：

1. 計算數據的協方差矩陣 covariance matrix
2. 計算協方差矩陣中的特徵值和特徵向量，最大的特徵值對應的特徵向量就是第一主成分，第二大的特徵值對應的特徵向量就是第二主成分。
3. 將數據投影到這些主成分上，得到降維後的數據。

### Multidimensional Scaling, MDS

## Assignment 5

