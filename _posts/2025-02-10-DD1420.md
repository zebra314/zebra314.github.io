---
title: "DD1420"
header:
  teaser: /assets/image/25/0210-tb.webp
tag: Exchange
math: true
toc: true
toc_sticky: true
---

## Course Information

Name：Foundations of Machine Learning

[Lecture Notes](https://dd1420.notion.site/DD1420-Lecture-Notes-b555e017345a4119950ce8fd67133275)

## Lesson 5

### 幾何學

### Dimensionality reduction

將數據的維度降低，複雜度降低，我們能更容易的分析數據，當資料的維度降低到只剩下幾個抽象特徵時，可以加快模型的訓練速度和大小，且當維度降低至2或3維時，也能更直觀的視覺化數據。

### Principal Component Analysis, PCA

主成份分析，一種經典的線性降維方法，目標是尋找一個最佳的線性正交投影 $\pi$，將一組 $d$ 維的數據降到 $k$ 維，$d > k$，使得投影後的數據在低維子空間中能夠保有最大的變異性。

這個子空間有 $k$ 維，由 $k$ 個正交的特徵向量 $u_1, \dots, u_k \in \mathbb{R}^n$ 張成，這些向量稱為主成分，滿足以下條件：

1. 正交：$u_i^T u_j = 0, i \neq j$，確保這些向量不會有重複的資訊，每個向量表示不同的方向。

2. 單位長度：$u_i^t u_i = \|u_i\|^2 = 1$，確保數據在投影後不會因為基底向量的長度變化而被放大或縮小。

變異性代表數據的分散程度，如果數據在某方向上的變異性較大，代表這個方向上的變化較多，不同的數據點有顯著區別，表示資訊較多、較豐富。

PCA有三個步驟：

1. 計算數據的協方差矩陣 covariance matrix
2. 計算協方差矩陣中的特徵值和特徵向量，最大的特徵值對應的特徵向量就是第一主成分，第二大的特徵值對應的特徵向量就是第二主成分。
3. 將數據投影到這些主成分上，得到降維後的數據。

### Multidimensional Scaling, MDS

## Lesson 6

**Kernel Methods**

Kernel methods are the methods to make linear models (like linear regression and SVMs) handle non-linear data.

The idea is to mapping data into a higher-dimensional space where a linear model can be applied. However, the mapping is not actually performed, instead, they use kernel functions to compute the inner product directly in the higher-dimensional space.

---

**Linear Regression**

The linear regression model tries to fit a linear function (a straight line) to the data, which minimizes the sum of squared errors between the predicted value and the true value.

The equation of the line is $y = \theta_0 + \theta_1 x$, and for each data point $(x_i, y_i)$, the model predicts $\hat{y}_i = \theta_0 + \theta_1 x_i$.

The error for each data point is $e_i = y_i - \hat{y}_i$, and the sum of squared errors is

$$
\begin{align}
E &= \Sigma_{i=1}^n (y_i - \hat{y}_i)^2 \\
  &= \Sigma_{i=1}^n (y_i - \theta_0 - \theta_1 x_i)^2 \\
  &= \Sigma_{i=1}^n (y_i - \theta^t x_i)^2 \\
\end{align}
$$

The reason to use the squared error is that it avoid the cancellation of positive and negative errors and also penalize large errors more compare to the small errors.

---

**Ridge Regression**

When trying to minimize the loss, we might meet the degenerate case: the solution is not unique because of too less data points, or too many features.

Ridge Regression can solve this by adding a regularization term to the loss function, which is the sum of squared weights, encouraging the weights to be small at the same time minimize the loss to prevent overfitting.

$$
E = \Sigma_{i=1}^n (y_i - \theta^t x_i)^2 + \lambda ||\theta||^2
$$

Also because of this term, we can always get a unique solution even if the data is not enough.

---

**Primal Solution**

Primal and dual solutions are two different ways to solve the same problem. The primal solution focuses on finding the weights $\theta$ directly.

$$
E = \Sigma_{i=1}^n (y_i - \theta^t x_i)^2 + \lambda ||\theta||^2
$$

Derive the loss function with respect to $\theta$ and set it to zero, we can get the optimal weights

$$
\theta = (X^t X + \lambda I_k)^{-1} X^t y
$$

- $X$ is the data matrix, $n \times k$
- $n$ is the number of samples, $k$ is the number of features.
- $X^t X$ is a $k \times k$ matrix.
- The inverse matrix $(X^t X + \lambda I_k)^{-1}$ is a $k \times k$ matrix with computational time complexity $O(k^3)$.
- The solution $\theta$ is a $k \times 1$ vector, which is in the feature space.

---

**Dual Solution**

If the dimensional of the data is very large (a lot of features), it will be very slow to calculate the inverse of the matrix in the primal solution. We use the dual solution to solve this problem.

The dual solution focuses on finding the Lagrange multipliers $\alpha$, which is the weight of each data point.

$$
\theta = X^t \alpha
$$

Bring this into the original ridge regression loss function, we can get the best $\alpha$.

$$
\alpha = (K + \lambda I_n)^{-1} y \\
\implies \theta = X^t (K + \lambda I_n)^{-1} y
$$

- $K = X X^t$, the Gram matrix, is a $n \times n$ matrix.
- The inverse matrix is a $n \times n$ matrix with computational time complexity $O(n^3)$.
- The solution $\alpha$ is a $n \times 1$ vector, which is in the sample space.

| Formulation | Primal Solution | Dual Solution |
| --- | --- | --- |
| Solution | $\theta = (X^t X + \lambda I_k)^{-1} X^t y$ | $\theta = X^t (K + \lambda I_n)^{-1} y$ |
| Space | Feature space | Sample space |
| Computational Complexity | $O(k^3)$ | $O(n^3)$ |
| Kernel Methods | Not practical in high-dimensional data | Yes |

---

**Polynomial Regression**

In previous linear regression, we assume the relationship between the input and output is linear. However, in some cases, the relationship is not linear, but polynomial.

It first maps the input $x$ to a higher-dimensional space, using $x \to \phi(x)$, then apply the linear regression in this space.

If we apply this to the primal and dual solution, they become:

$$
Primal: \theta = (\Phi^t \Phi + \lambda I_k)^{-1} \Phi^t y \\
Dual: \theta = \Phi^t (\Phi \Phi^t + \lambda I_n)^{-1} y
$$

---

**Kernel Functions**

Kernel function: $k(x, y) = <\phi(x), \phi(y)>$
It is defined as the inner product of the mapped data in the transformed space.

**Kernel Trick**

It will be very expensive to calculate the $\Phi$ when the data is very high-dimensional. Instead of calculating the $\Phi$ directly, we can calculate the kernel function $k(x, y)$ to get the inner product of the mapped data in the transformed space without actually mapping the data.


**Polynomial Kernel**

$$
k(x, y) = (x^t y + c)^d
$$

**Gaussian Kernel**

$$
k(x, y) = \exp(-\frac{||x - y||^2}{2\sigma^2})
$$

which maps the data to an infinite-dimensional space.

**Periodic Kernel**

## Lesson 7

## Lesson 8

### 8.2

**Information Content**

Also known as Shannon information, measures how surprising an event is (the uncertainty).

$$
I(x) = log_2 \frac{1}{P(x)} = -log_2 P(x)
$$

Why $log_2$?

- Monotonic
- $P(x)=1 \implies I(x)=0$  
  when the event will definitely happen, the information content is 0, meaning that it is not surprising at all.
- $I(x, y) = I(x) + I(y)$

**Entropy**

The expected information content of a random variable $X$, it describes the expected uncertainty in a probability mass function.

$$
H(X) = \mathbb{E}_{x \sim P_X} [I(x)] = -\sum_{x \in \Omega} P_X(x) log_2 P_X(x)
$$

The unit of entropy is bit or dits if using $log_{10}$ or nats if using $log_e$.

**Perplexity**

Perplexity is another way to express the entropy (the uncertainty).

The amount of surprise of the model when predicting a sample.

$$
PP(X) = 2^{H(x)}
$$

**Cross Entropy**

The expected information content of a probability mass function $Q_X$ with respect to another probability mass function $P_X$.

$$
\begin{align}
H(P_X, Q_X) &= \mathbb{E}_{x \sim P_X} [I(x, Q_X)] \\
&= -\sum_{x \in \Omega} P_X(x) \log Q_X(x)
\end{align}
$$

**Kullback-Leibler Divergence**

The KLD is a measure of how much one probability distribution differs from another.

It compares the true distribution (the data $P_X$) with the approximate distribution (the model $Q_X$) by calculating the difference between the cross entropy and the entropy.

$$
\begin{align*}
D_{\text{KL}}(P_{\pmb x}\|Q_{\pmb x})
&= H(P_X, Q_X) - H(P_X) \\
& = \mathbb{E}_{x\sim \mathcal P_{\pmb x}}\left[\log\frac{P_{\pmb x}(x)}{Q_{\pmb x}(x)}\right]\\
& = \sum_{x\in \Omega}   P_{\pmb x}(x) \log \frac{P_{\pmb x}(x)}{Q_{\pmb x}(x)}
\end{align*}
$$

- $D_{\text{KL}}(P_{\pmb x}\|Q_{\pmb x}) \geq 0 \quad \forall P_{\pmb x}, Q_{\pmb x}$
- $D_{\text{KL}}(P_{\pmb x}\|Q_{\pmb x}) = 0 \iff P_{\pmb x} = Q_{\pmb x}$

Why it is not a distance?

- Not symmetric:  
  $D_{\text{KL}}(P_{\pmb x}\|Q_{\pmb x}) \neq D_{\text{KL}}(Q_{\pmb x}\|P_{\pmb x})$
- Not triangle inequality:  
  $D_{\text{KL}}(P_{\pmb x}\|Q_{\pmb x}) + D_{\text{KL}}(Q_{\pmb x}\|R_{\pmb x}) \ngeq D_{\text{KL}}(P_{\pmb x}\|R_{\pmb x})$

Forward KL: $D_{\text{KL}}(P \parallel Q)$

- Approximates $P$ with $Q$
- Penalizes: $Q$ heavily if it assigns low probability to regions where $P$ has high probability ($\log \frac{P(x)}{Q(x)}$ becomes very large).
- Behavior: Mass-covering, $Q$ can't be too small in any region where $P$ supports, $Q$ spreads probability mass to cover *all* regions where $P$ exists.

Reverse KL: $D_{\text{KL}}(Q \parallel P)$

- Approximates $Q$ with $P$
- Penalizes: $Q$ if it assigns high probability to regions where $P$ has low probability ($\log \frac{Q(x)}{P(x)}$ becomes very large).
- Behavior: Mode-seeking, $Q$ focuses on matching a single mode (peak) of $P$, it needs to be small in regions where $P$ is small.

### 8.3
